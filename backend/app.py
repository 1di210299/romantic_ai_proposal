"""
Main Flask application for Romantic AI Proposal System.
A personalized chatbot that guides your loved one through a relationship quiz.

Las preguntas se generan din√°micamente usando OpenAI + RAG (Retrieval-Augmented Generation).
"""

import os
import json
import logging
import sys
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from pathlib import Path
import uuid
from datetime import datetime
from openai import OpenAI
from services.rag_service import get_rag_service
from prompts.question_generator_prompt import get_question_generator_prompt
from services.chatbot import generate_conversational_response

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('app.log')
    ]
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

logger.info("üöÄ Iniciando aplicaci√≥n Flask...")

app = Flask(__name__)
CORS(app)

logger.info("‚úÖ Flask y CORS configurados")

# Configuration
app.config['SECRET_KEY'] = os.getenv('SECRET_KEY', 'dev-secret-key-change-in-production')
app.config['DEBUG'] = os.getenv('FLASK_DEBUG', 'True') == 'True'

logger.info(f"üîß Configuraci√≥n - DEBUG: {app.config['DEBUG']}")

# OpenAI client
openai_api_key = os.getenv('OPENAI_API_KEY')
if openai_api_key:
    logger.info("‚úÖ OpenAI API key encontrada")
    openai_client = OpenAI(api_key=openai_api_key)
else:
    logger.error("‚ùå OpenAI API key NO encontrada!")
    openai_client = None

# RAG Service (inicializado despu√©s de cargar mensajes)
rag_service = None
_rag_initialized = False

def ensure_rag_initialized():
    """Asegura que el RAG service est√© inicializado."""
    global rag_service, _rag_initialized
    
    if _rag_initialized and rag_service is not None:
        return rag_service
    
    try:
        logger.info("üì° Inicializando RAG Service...")
        print("üì° Inicializando RAG Service...")
        
        # Crear instancia del RAG service
        rag_service = get_rag_service(os.getenv('OPENAI_API_KEY'))
        
        # Cargar mensajes regulares
        all_messages = load_all_messages()
        logger.info(f"üì• {len(all_messages)} mensajes cargados para RAG")
        
        # Cargar chunks prioritarios de transcripci√≥n
        from services.spaces_loader import SpacesDataLoader
        spaces_loader = SpacesDataLoader()
        priority_messages = spaces_loader.download_priority_transcription()
        logger.info(f"üéØ {len(priority_messages)} chunks prioritarios cargados")
        
        # Construir √≠ndice (o cargar desde cache) con prioridades
        rag_service.build_index(all_messages, force_rebuild=False, priority_messages=priority_messages)
        
        # Mostrar estad√≠sticas
        stats = rag_service.get_statistics()
        logger.info(f"‚úÖ RAG inicializado - Chunks: {stats.get('total_chunks', 0):,}")
        print(f"‚úÖ RAG inicializado - {stats.get('total_chunks', 0):,} chunks")
        
        _rag_initialized = True
        return rag_service
        
    except Exception as e:
        logger.error(f"‚ùå Error inicializando RAG: {e}")
        print(f"‚ùå Error inicializando RAG: {e}")
        rag_service = None
        return None

# Conversation data path - resolver ruta absoluta
CONVERSATION_PATH = os.getenv('CONVERSATION_DATA_PATH', '../karemramos_1184297046409691')
# Convertir a ruta absoluta desde la ubicaci√≥n del script
CONVERSATION_PATH = Path(__file__).parent / CONVERSATION_PATH
CONVERSATION_PATH = CONVERSATION_PATH.resolve()

logger.info(f"üìÇ Ruta de conversaci√≥n: {CONVERSATION_PATH}")
print(f"üìÇ Ruta de conversaci√≥n: {CONVERSATION_PATH}")

# Global state (in production, use a database)
# Estructura: {session_id: {questions, current_index, answers, etc}}
quiz_sessions = {}


def load_messages_sample(max_messages: int = 1000):
    """Carga una muestra de mensajes para an√°lisis."""
    print(f"üìÇ Cargando mensajes desde {CONVERSATION_PATH}...")
    
    all_messages = []
    conversation_dir = Path(CONVERSATION_PATH)
    
    # Leer todos los archivos de mensajes
    for msg_file in sorted(conversation_dir.glob('message_*.json')):
        try:
            with open(msg_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                messages = data.get('messages', [])
                all_messages.extend(messages)
                
                # Limitar para no cargar todo
                if len(all_messages) >= max_messages:
                    break
        except Exception as e:
            print(f"‚ö†Ô∏è  Error leyendo {msg_file.name}: {e}")
    
    # Ordenar por timestamp
    all_messages.sort(key=lambda x: x.get('timestamp_ms', 0))
    
    # Tomar solo los m√°s recientes
    recent_messages = all_messages[-max_messages:] if len(all_messages) > max_messages else all_messages
    
    print(f"‚úÖ {len(recent_messages)} mensajes cargados")
    return recent_messages


def load_all_messages():
    """Carga TODOS los mensajes disponibles para RAG."""
    print(f"üìÇ Ruta de conversaci√≥n: {CONVERSATION_PATH}")
    
    all_messages = []
    
    # PRIMERO: Intentar cargar desde DigitalOcean Spaces
    try:
        from services.spaces_loader import load_messages_from_spaces
        print("üåê Intentando cargar desde DigitalOcean Spaces...")
        
        spaces_messages = load_messages_from_spaces()
        if spaces_messages:
            print(f"‚úÖ {len(spaces_messages)} mensajes cargados desde Spaces")
            return spaces_messages
        else:
            print("‚ö†Ô∏è  Spaces no disponible, usando m√©todo local...")
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Error con Spaces: {e}")
        print("üîÑ Continuando con b√∫squeda local...")
    
    # FALLBACK: Buscar archivos locales
    possible_paths = [
        Path(CONVERSATION_PATH),  # Ruta configurada
        Path("../karemramos_1184297046409691"),  # Relativa desde backend
        Path("./karemramos_1184297046409691"),   # Relativa desde directorio actual
        Path("/workspace/karemramos_1184297046409691"),  # DigitalOcean workspace
        Path(__file__).parent.parent / "karemramos_1184297046409691",  # Desde ra√≠z del proyecto
        Path("../data"),  # Backup en carpeta data desde backend
        Path("./data"),   # Backup en carpeta data desde directorio actual
        Path(__file__).parent.parent / "data"  # Backup en data desde ra√≠z
    ]
    
    conversation_dir = None
    
    # Probar cada ruta posible
    for path in possible_paths:
        print(f"üîç Probando ruta: {path.resolve()}")
        if path.exists():
            conversation_dir = path
            print(f"‚úÖ Directorio encontrado: {conversation_dir.resolve()}")
            break
    
    if not conversation_dir:
        print("‚ùå No se encontr√≥ el directorio de conversaci√≥n en ninguna ubicaci√≥n")
        print("üìÅ Contenido del directorio actual:")
        current_dir = Path(".")
        for item in current_dir.iterdir():
            print(f"  - {item.name}")
        
        # Tambi√©n mostrar el directorio padre
        print("üìÅ Contenido del directorio padre:")
        parent_dir = Path("..").resolve()
        try:
            for item in parent_dir.iterdir():
                print(f"  - {item.name}")
        except Exception as e:
            print(f"‚ùå Error listando directorio padre: {e}")
        
        return []
    
    # Buscar archivos JSON
    json_files = list(conversation_dir.glob('message_*.json'))
    print(f"üîç Archivos JSON encontrados: {len(json_files)}")
    
    if not json_files:
        print("‚ùå No se encontraron archivos message_*.json")
        print("üìÅ Contenido del directorio de conversaci√≥n:")
        try:
            for item in conversation_dir.iterdir():
                print(f"  - {item.name}")
        except Exception as e:
            print(f"‚ùå Error listando directorio: {e}")
        return []
    
    # Leer todos los archivos de mensajes
    for msg_file in sorted(json_files):
        try:
            print(f"üìñ Leyendo {msg_file.name}...")
            with open(msg_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                messages = data.get('messages', [])
                all_messages.extend(messages)
                print(f"  ‚úÖ {len(messages)} mensajes cargados desde {msg_file.name}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error leyendo {msg_file.name}: {e}")
    
    # Agregar archivos adicionales de transcripci√≥n e historia
    additional_messages = load_additional_story_files()
    all_messages.extend(additional_messages)
    
    print(f"‚úÖ {len(all_messages)} mensajes totales cargados para RAG")
    return all_messages


def load_additional_story_files():
    """Carga archivos adicionales de historia y transcripci√≥n para enriquecer el RAG."""
    additional_messages = []
    
    # Rutas de archivos adicionales
    data_files = [
        "data/historia_completa_transcripcion.txt",
        "data/timeline_estructurado.json"
    ]
    
    base_path = Path(__file__).parent
    
    for file_path in data_files:
        full_path = base_path / file_path
        try:
            if full_path.exists():
                if file_path.endswith('.txt'):
                    # Cargar archivo de texto
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # Crear mensaje simulado para RAG
                        additional_messages.append({
                            'sender_name': 'historia_transcripcion',
                            'content': content,
                            'timestamp_ms': int(datetime.now().timestamp() * 1000),
                            'type': 'historia_completa'
                        })
                        print(f"  ‚úÖ Historia completa cargada desde {file_path}")
                        
                elif file_path.endswith('.json'):
                    # Cargar archivo JSON
                    with open(full_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        # Convertir JSON a texto para RAG
                        content = json.dumps(data, indent=2, ensure_ascii=False)
                        additional_messages.append({
                            'sender_name': 'timeline_estructurado',
                            'content': content,
                            'timestamp_ms': int(datetime.now().timestamp() * 1000),
                            'type': 'timeline_estructurado'
                        })
                        print(f"  ‚úÖ Timeline estructurado cargado desde {file_path}")
            else:
                print(f"‚ö†Ô∏è  Archivo no encontrado: {full_path}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Error cargando {file_path}: {e}")
    
    return additional_messages


def format_messages_for_ai(messages: list) -> str:
    """Formatea mensajes para enviar a OpenAI."""
    formatted = []
    for msg in messages:
        sender = msg.get('sender_name', 'Unknown')
        content = msg.get('content', '')
        timestamp = msg.get('timestamp_ms', 0)
        
        if content:
            date = datetime.fromtimestamp(timestamp / 1000).strftime('%Y-%m-%d')
            formatted.append(f"[{date}] {sender}: {content}")
    
    return "\n".join(formatted[-200:])  # √öltimos 200 mensajes


def generate_single_question_with_openai(messages: list, question_number: int, previous_questions: list = None) -> dict:
    """
    Genera UNA pregunta espec√≠fica usando OpenAI + RAG.
    Usa b√∫squeda sem√°ntica para encontrar contexto relevante en los mensajes.
    """
    # Asegurar que RAG est√© inicializado - OBLIGATORIO, sin fallbacks
    current_rag = ensure_rag_initialized()
    if not current_rag:
        print("‚ùå RAG service no disponible - no se pueden generar preguntas sin datos reales")
        return None
    
    print(f"ü§ñ Generando pregunta #{question_number} con OpenAI + RAG...")
    
    # üîç PASO 1: Temas ultra espec√≠ficos para b√∫squeda contextual profunda
    # CADA TEMA busca contextos √∫nicos e irrepetibles de la relaci√≥n  
    question_topics = [
        "jajaja risa chistoso gracioso divertido re√≠r humor broma chiste",  # Momentos espec√≠ficos de humor
        "lugar salir ir vamos fuimos parque casa restaurante cine caf√©",  # Lugares y experiencias espec√≠ficas
        "comida comer hambre desayuno almuerzo cena pizza hamburguesa",  # Contextos gastron√≥micos √∫nicos
        "pel√≠cula ver Netflix serie programa televisi√≥n pel√≠cula favorita",  # Entertainment espec√≠fico visto
        "futuro planes casarnos hijos familia sue√±os juntos siempre",  # Planes concretos mencionados
        "problema enojado pelea discusi√≥n triste mal perd√≥n disculpa",  # Conflictos y resoluciones espec√≠ficas
        "regalo sorpresa detalle especial rom√°ntico cumplea√±os aniversario",  # Momentos rom√°nticos √∫nicos
        "canci√≥n m√∫sica artista bailar escuchar spotify reproducir",  # Referencias musicales espec√≠ficas
        "amigos familia conocer presentar mam√° pap√° hermana hermano",  # Contextos sociales/familiares
        "primera vez beso te amo inicio conocimos empezamos",  # Hitos relacionales espec√≠ficos
        "trabajo estudios universidad clase profesor examen tarea",  # Contexto acad√©mico/profesional  
        "viaje vacaciones playa monta√±a ciudad pa√≠s avi√≥n carro",  # Experiencias de viaje espec√≠ficas
        "enfermo dolor cabeza medicina doctor hospital cuidar",  # Momentos de cuidado mutuo
        "noche dormir sue√±o despertar ma√±ana tarde madrugada",  # Rutinas y horarios espec√≠ficos
        "foto selfie imagen bonita hermosa guapo lindo",  # Contextos visuales/est√©ticos
    ]
    
    # Usar el √≠ndice exacto de la pregunta (sin rotar) para variedad
    topic_index = min(question_number - 1, len(question_topics) - 1)
    search_query = question_topics[topic_index]
    
    print(f"üîç B√∫squeda RAG: '{search_query}'...")
    
    # Buscar chunks relevantes
    relevant_chunks = current_rag.search(search_query, k=15)
    
    # Extraer todos los mensajes de los chunks relevantes
    relevant_messages = []
    for chunk in relevant_chunks:
        relevant_messages.extend(chunk['messages_in_chunk'])
    
    print(f"üìö Encontrados {len(relevant_messages)} mensajes relevantes para el tema")
    
    # üìä PASO 2: AN√ÅLISIS CONTEXTUAL ULTRA PROFUNDO
    print(f"üî¨ Analizando {len(relevant_messages)} mensajes para encontrar contextos √∫nicos e irrepetibles...")
    
    # Filtrar mensajes por calidad y relevancia
    high_quality_messages = []
    for msg in relevant_messages:
        content = msg.get('content', '').strip()
        if len(content) > 10 and len(content) < 300:  # Mensajes de longitud √≥ptima
            high_quality_messages.append(msg)
    
    print(f"üìã {len(high_quality_messages)} mensajes de alta calidad seleccionados para an√°lisis profundo")
    
    # Recopilar todos los mensajes completos para an√°lisis detallado
    detailed_messages = []
    word_frequency = {}
    phrase_patterns = {}
    unique_contexts = {}
    
    for msg in relevant_messages:
        content = msg.get('content', '')
        sender = msg.get('sender_name', 'Unknown')
        timestamp = msg.get('timestamp_ms', 0)
        
        if not content.strip():
            continue
            
        date_str = datetime.fromtimestamp(timestamp / 1000).strftime('%d/%m/%Y %H:%M') if timestamp else 'fecha desconocida'
        
        # Guardar mensaje completo para an√°lisis contextual
        detailed_messages.append({
            'sender': sender,
            'content': content,
            'date': date_str,
            'timestamp': timestamp,
            'length': len(content)
        })
        
        # An√°lisis de frecuencia de palabras (din√°mico)
        words = content.lower().split()
        for word in words:
            if len(word) > 2:  # Ignorar palabras muy cortas
                word_frequency[word] = word_frequency.get(word, 0) + 1
        
        # Detectar patrones de frases (bigramas y trigramas)
        for i in range(len(words) - 1):
            bigram = ' '.join(words[i:i+2])
            phrase_patterns[bigram] = phrase_patterns.get(bigram, 0) + 1
            
        for i in range(len(words) - 2):
            trigram = ' '.join(words[i:i+3])
            phrase_patterns[trigram] = phrase_patterns.get(trigram, 0) + 1
    
    # Filtrar y ordenar por relevancia
    significant_words = {word: count for word, count in word_frequency.items() 
                        if count >= 2 and word not in ['que', 'para', 'con', 'por', 'una', 'del', 'las', 'los', 'pero', 'como', 'm√°s', 'ser', 'hay', 'muy', 'fue', 'sus', 'son', 'ese', 'esa']}
    
    significant_phrases = {phrase: count for phrase, count in phrase_patterns.items() 
                          if count >= 2 and len(phrase.split()) >= 2}
    
    # Ordenar por frecuencia
    top_words = sorted(significant_words.items(), key=lambda x: x[1], reverse=True)[:10]
    top_phrases = sorted(significant_phrases.items(), key=lambda x: x[1], reverse=True)[:10]
    
    print(f"üìà Palabras m√°s frecuentes: {[f'{word}({count})' for word, count in top_words[:5]]}")
    print(f"üìà Frases m√°s frecuentes: {[f'{phrase}({count})' for phrase, count in top_phrases[:3]]}")
    
    # Usar los nuevos datos din√°micos en lugar de hardcodeados
    dynamic_nicknames = top_words  # Las palabras m√°s frecuentes pueden incluir apodos
    dynamic_phrases = top_phrases
    dynamic_locations = [(word, count) for word, count in top_words if any(loc in word for loc in ['casa', 'parque', 'cine', 'restaurante', 'lugar', 'caf√©', 'playa'])]
    
    # üìù Crear contexto rico con mensajes reales detallados
    # Ordenar mensajes por timestamp para obtener fechas correctas
    sorted_messages = sorted(detailed_messages, key=lambda x: x.get('timestamp', 0))
    first_date = sorted_messages[0]['date'] if sorted_messages else "fecha no disponible"
    last_date = sorted_messages[-1]['date'] if sorted_messages else "fecha no disponible"
    
    # Formatear los mensajes m√°s relevantes para an√°lisis
    examples_text = "\n".join([
        f"- [{msg['date']}] {msg['sender']}: \"{msg['content']}\""
        for msg in detailed_messages[:15]  # Los primeros 15 mensajes m√°s relevantes
    ])
    
    previous_qs = "\n".join([f"- {q.get('question', '')}" for q in (previous_questions or [])]) if previous_questions else "ninguna"
    
    # ü§ñ PASO 3: Generar prompt usando an√°lisis din√°mico
    prompt = get_question_generator_prompt(
        top_nicknames=dynamic_nicknames,
        top_phrases=dynamic_phrases,
        top_locations=dynamic_locations,
        examples_text=examples_text,
        last_date=last_date,
        previous_qs=previous_qs,
        question_number=question_number
    )

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "Eres un experto en crear quizzes rom√°nticos personalizados. Respondes SIEMPRE en JSON v√°lido sin formato markdown."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        tokens_used = response.usage.total_tokens
        
        print(f"‚úÖ Pregunta generada ({tokens_used} tokens, ~${tokens_used * 0.000005:.4f})")
        
        # Validar que las opciones no se repitan con preguntas anteriores
        new_options = set(result.get('options', []))
        if previous_questions:
            for prev_q in previous_questions:
                prev_options = set(prev_q.get('options', []))
                overlapping = new_options & prev_options
                if overlapping:
                    print(f"‚ö†Ô∏è ADVERTENCIA: Opciones repetidas detectadas: {overlapping}")
        
        question_data = {
            "question": result.get('question', ''),
            "options": result.get('options', []),
            "correct_answers": result.get('correct_answers', []),
            "hints": result.get('hints', []),
            "success_message": result.get('success_message', 'Correcto.'),
            "category": result.get('category', 'general'),
            "difficulty": result.get('difficulty', 'medium'),
            "data_source": result.get('data_source', 'Datos de conversaci√≥n')
        }
        
        print(f"üìã Pregunta: {question_data['question']}")
        print(f"üéØ Respuestas correctas: {question_data['correct_answers']}")
        print(f"üìä Fuente: {question_data['data_source']}")
        
        return question_data
    
    except Exception as e:
        print(f"‚ùå Error generando pregunta con OpenAI: {e}")
        import traceback
        traceback.print_exc()
        
        # Sin fallbacks - retornar None si falla
        print("‚ùå No se pudo generar pregunta - sin fallbacks disponibles")
        return None


# Health check endpoint for monitoring and Docker
@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint for deployment verification"""
    logger.info("üîç Health check endpoint called")
    try:
        current_rag = ensure_rag_initialized()
        rag_enabled = current_rag is not None
        total_messages = len(current_rag.chunk_texts) if current_rag else 0
        
        logger.info(f"‚úÖ Health check successful - RAG: {rag_enabled}, Messages: {total_messages}")
        
        return jsonify({
            "status": "ok",
            "timestamp": datetime.now().isoformat(),
            "rag_enabled": rag_enabled,
            "total_messages": total_messages,
            "environment": os.environ.get('FLASK_ENV', 'development'),
            "port": os.environ.get('BACKEND_PORT', '5000')
        })
    except Exception as e:
        logger.error(f"‚ùå Health check failed: {str(e)}")
        return jsonify({
            "status": "error", 
            "error": str(e)
        }), 500

def analyze_conversation_data():
    """Analiza los datos reales de conversaci√≥n cargados"""
    try:
        from services.spaces_loader import load_messages_from_spaces
        
        print("üìä Analizando datos reales de conversaci√≥n...")
        
        # Intentar cargar mensajes desde Spaces o local
        messages = []
        try:
            messages = load_messages_from_spaces()
            print(f"‚úÖ Mensajes cargados desde Spaces: {len(messages)}")
        except:
            print("‚ö†Ô∏è No se pudieron cargar desde Spaces, usando datos locales...")
            # Cargar desde archivos locales como fallback
            pass
        
        if not messages:
            return None
            
        # AN√ÅLISIS REAL DE DATOS
        total_messages = len(messages)
        
        # Extraer fechas de los mensajes
        dates = []
        message_times = []
        senders = {}
        content_analysis = {
            'total_chars': 0,
            'romantic_keywords': 0,
            'emojis': {},
            'longest_message': 0,
            'conversations_by_date': {}
        }
        
        for msg in messages:
            try:
                # An√°lisis de fecha y hora
                if 'timestamp_ms' in msg:
                    timestamp = datetime.fromtimestamp(msg['timestamp_ms'] / 1000)
                    dates.append(timestamp)
                    message_times.append(timestamp.hour)
                    date_key = timestamp.strftime('%Y-%m')
                    content_analysis['conversations_by_date'][date_key] = content_analysis['conversations_by_date'].get(date_key, 0) + 1
                
                # An√°lisis de remitente
                sender = msg.get('sender_name', 'Unknown')
                senders[sender] = senders.get(sender, 0) + 1
                
                # An√°lisis de contenido
                content = msg.get('content', '')
                if content:
                    content_analysis['total_chars'] += len(content)
                    content_analysis['longest_message'] = max(content_analysis['longest_message'], len(content))
                    
                    # Buscar palabras rom√°nticas
                    romantic_words = ['amor', 'te amo', 'mi vida', 'coraz√≥n', 'besitos', 'hermosa', 'princesa', 'mi amor', 'baby', 'cari√±o']
                    content_lower = content.lower()
                    for word in romantic_words:
                        if word in content_lower:
                            content_analysis['romantic_keywords'] += 1
                    
                    # Contar emojis
                    import re
                    emoji_pattern = re.compile("["
                        "\U0001F600-\U0001F64F"  # emoticons
                        "\U0001F300-\U0001F5FF"  # symbols & pictographs
                        "\U0001F680-\U0001F6FF"  # transport & map symbols
                        "\U0001F1E0-\U0001F1FF"  # flags (iOS)
                        "\U00002702-\U000027B0"
                        "\U000024C2-\U0001F251"
                        "]+", flags=re.UNICODE)
                    
                    emojis_found = emoji_pattern.findall(content)
                    for emoji in emojis_found:
                        content_analysis['emojis'][emoji] = content_analysis['emojis'].get(emoji, 0) + 1
                        
            except Exception as e:
                print(f"Error procesando mensaje: {e}")
                continue
        
        # Calcular estad√≠sticas
        if dates:
            dates.sort()
            total_days = (dates[-1] - dates[0]).days + 1
            avg_messages_per_day = round(total_messages / total_days, 1) if total_days > 0 else 0
            
            # Hora m√°s activa
            most_active_hour = max(set(message_times), key=message_times.count) if message_times else 12
            
            # Score de sentimiento basado en palabras rom√°nticas
            sentiment_score = min(10, round((content_analysis['romantic_keywords'] / total_messages) * 100 + 5, 1))
            
            # Top emojis
            top_emojis = sorted(content_analysis['emojis'].items(), key=lambda x: x[1], reverse=True)[:5]
            top_emojis_list = [emoji[0] for emoji in top_emojis] if top_emojis else ['‚ù§', 'ÔøΩ', 'üíú']
            
            # Fases de la relaci√≥n basadas en datos reales
            monthly_data = sorted(content_analysis['conversations_by_date'].items())
            phases = []
            if len(monthly_data) >= 3:
                third = len(monthly_data) // 3
                # Forzar que el inicio sea marzo 2025
                start_period = "2025-03"
                phases = [
                    {
                        "phase": "Inicio",
                        "messages": sum([monthly_data[i][1] for i in range(third)]),
                        "period": f"{start_period} - {monthly_data[third-1][0]}"
                    },
                    {
                        "phase": "Creciendo", 
                        "messages": sum([monthly_data[i][1] for i in range(third, third*2)]),
                        "period": f"{monthly_data[third][0]} - {monthly_data[third*2-1][0]}"
                    },
                    {
                        "phase": "Consolidaci√≥n",
                        "messages": sum([monthly_data[i][1] for i in range(third*2, len(monthly_data))]),
                        "period": f"{monthly_data[third*2][0]} - {monthly_data[-1][0]}"
                    }
                ]
            
            return {
                "totalMessages": total_messages,
                "totalDays": total_days,
                "avgMessagesPerDay": avg_messages_per_day,
                "longestConversation": content_analysis['longest_message'],
                "mostActiveHour": most_active_hour,
                "sentimentScore": sentiment_score,
                "relationshipPhases": phases,
                "topEmojis": top_emojis_list,
                "specialMoments": content_analysis['romantic_keywords'],
                "senderDistribution": senders,
                "totalChars": content_analysis['total_chars'],
                "firstMessage": dates[0].strftime('%Y-%m-%d') if dates else None,
                "lastMessage": dates[-1].strftime('%Y-%m-%d') if dates else None
            }
        
        return None
        
    except Exception as e:
        print(f"Error en an√°lisis: {e}")
        import traceback
        traceback.print_exc()
        return None

@app.route('/api/relationship-stats', methods=['GET'])
def get_relationship_stats():
    """Generate relationship statistics from real conversation data (with enhanced AI analysis)"""
    try:
        # Verificar si se solicita an√°lisis forzado
        force_analysis = request.args.get('force', 'false').lower() == 'true'
        
        # üöÄ OPTIMIZACI√ìN: Intentar cargar desde cache primero (si no es forzado)
        if not force_analysis:
            from services.stats_cache import get_stats_cache
            stats_cache = get_stats_cache()
            
            # Verificar cache mejorado primero
            enhanced_cache_file = Path("../cache/enhanced_relationship_stats.json")
            if enhanced_cache_file.exists():
                try:
                    with open(enhanced_cache_file, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    
                    cached_at = datetime.fromisoformat(cache_data.get('cached_at', ''))
                    age_hours = (datetime.now() - cached_at).total_seconds() / 3600
                    
                    if age_hours <= 24:  # Cache v√°lido por 24 horas
                        enhanced_stats = cache_data.get('stats', {})
                        enhanced_stats["cache_hit"] = True
                        enhanced_stats["cache_age_hours"] = round(age_hours, 1)
                        print(f"‚ö° Estad√≠sticas mejoradas servidas desde cache ({age_hours:.1f}h)")
                        return jsonify(enhanced_stats)
                except Exception as e:
                    print(f"‚ö†Ô∏è Error leyendo cache mejorado: {e}")
            
            # Verificar cache tradicional
            cached_stats = stats_cache.get_cached_stats()
            if cached_stats:
                cached_stats["generated_at"] = datetime.now().isoformat()
                cached_stats["data_source"] = "cached_analysis"
                cached_stats["cache_hit"] = True
                print("‚ö° Estad√≠sticas tradicionales servidas desde cache")
                return jsonify(cached_stats)
        
        # Usar an√°lisis mejorado con IA
        print("ü§ñ Ejecutando an√°lisis mejorado con IA...")
        try:
            # Importar y ejecutar el analizador mejorado
            sys.path.append('..')
            from enhanced_stats_analyzer import EnhancedStatsAnalyzer
            
            analyzer = EnhancedStatsAnalyzer()
            enhanced_stats = analyzer.generate_enhanced_stats()
            
            if enhanced_stats:
                enhanced_stats["cache_hit"] = False
                enhanced_stats["analysis_type"] = "enhanced_ai_powered"
                print("‚úÖ An√°lisis mejorado completado")
                return jsonify(enhanced_stats)
                
        except ImportError as e:
            print(f"‚ö†Ô∏è No se pudo importar analizador mejorado: {e}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error en an√°lisis mejorado: {e}")
            import traceback
            traceback.print_exc()
        
        # Fallback al an√°lisis tradicional
        print("üìä Fallback a an√°lisis tradicional...")
        real_stats = analyze_conversation_data()
        
        if real_stats:
            real_stats["generated_at"] = datetime.now().isoformat()
            real_stats["data_source"] = "traditional_analysis"
            real_stats["cache_hit"] = False
            current_rag = ensure_rag_initialized()
            if current_rag and hasattr(current_rag, 'chunk_texts'):
                real_stats["rag_chunks"] = len(current_rag.chunk_texts)
            
            return jsonify(real_stats)
        
        # Fallback final: estimaci√≥n basada en RAG
        current_rag = ensure_rag_initialized()
        if current_rag and hasattr(current_rag, 'chunk_texts'):
            total_chunks = len(current_rag.chunk_texts)
            estimated_messages = total_chunks * 5
            
            fallback_stats = {
                "totalMessages": estimated_messages,
                "totalDays": 800,
                "avgMessagesPerDay": round(estimated_messages / 800, 1),
                "connectionScore": 8.5,
                "avgResponseTime": "15min",
                "relationshipPhases": [
                    {"phase": "Inicio", "messages": int(estimated_messages * 0.2), "period": "Primeros meses"},
                    {"phase": "Creciendo", "messages": int(estimated_messages * 0.4), "period": "Desarrollo"},
                    {"phase": "Consolidaci√≥n", "messages": int(estimated_messages * 0.4), "period": "Actualidad"}
                ],
                "topEmojis": ['‚ù§Ô∏è', 'üòò', 'üíú', 'üòç', 'ü•∞'],
                "specialMoments": int(estimated_messages * 0.05),
                "generated_at": datetime.now().isoformat(),
                "data_source": "rag_estimation_fallback",
                "cache_hit": False,
                "rag_chunks": total_chunks
            }
            
            return jsonify(fallback_stats)
        
        # Sin datos disponibles
        return jsonify({
            "error": "No conversation data available",
            "message": "No analysis method succeeded"
        }), 503
            
    except Exception as e:
        return jsonify({
            "error": str(e),
            "data_source": "error_fallback"
        }), 500

@app.route('/api/relationship-stats/regenerate', methods=['POST'])
def regenerate_relationship_stats():
    """Fuerza la regeneraci√≥n de estad√≠sticas con an√°lisis mejorado"""
    try:
        print("üîÑ Forzando regeneraci√≥n de estad√≠sticas mejoradas...")
        
        # Importar y ejecutar el analizador mejorado
        sys.path.append('..')
        from enhanced_stats_analyzer import EnhancedStatsAnalyzer
        
        analyzer = EnhancedStatsAnalyzer()
        enhanced_stats = analyzer.generate_enhanced_stats()
        
        if enhanced_stats:
            enhanced_stats["cache_hit"] = False
            enhanced_stats["regenerated_at"] = datetime.now().isoformat()
            enhanced_stats["analysis_type"] = "enhanced_ai_forced_regeneration"
            
            # Limpiar cache antiguo
            try:
                from services.stats_cache import get_stats_cache
                stats_cache = get_stats_cache()
                stats_cache.clear_cache()
                print("üóëÔ∏è Cache anterior limpiado")
            except:
                pass
            
            print("‚úÖ Estad√≠sticas regeneradas exitosamente")
            return jsonify({
                "success": True,
                "message": "Estad√≠sticas regeneradas con an√°lisis mejorado",
                "stats": enhanced_stats
            })
        else:
            return jsonify({
                "success": False,
                "error": "No se pudieron generar estad√≠sticas mejoradas"
            }), 500
            
    except Exception as e:
        print(f"‚ùå Error regenerando estad√≠sticas: {e}")
        import traceback
        traceback.print_exc()
        
        return jsonify({
            "success": False,
            "error": str(e),
            "message": "Error durante la regeneraci√≥n de estad√≠sticas"
        }), 500


@app.route('/api/cache/stats-info', methods=['GET'])
def get_stats_cache_info():
    """Obtiene informaci√≥n del cache de estad√≠sticas"""
    try:
        from services.stats_cache import get_stats_cache
        stats_cache = get_stats_cache()
        
        cache_info = stats_cache.get_cache_info()
        return jsonify({
            "cache_info": cache_info,
            "success": True
        })
    except Exception as e:
        return jsonify({
            "error": str(e),
            "success": False
        }), 500


@app.route('/api/cache/clear-stats', methods=['POST'])
def clear_stats_cache():
    """Limpia el cache de estad√≠sticas para forzar rec√°lculo"""
    try:
        from services.stats_cache import get_stats_cache
        stats_cache = get_stats_cache()
        
        stats_cache.clear_cache()
        return jsonify({
            "message": "Cache de estad√≠sticas limpiado",
            "success": True
        })
    except Exception as e:
        return jsonify({
            "error": str(e),
            "success": False
        }), 500


@app.route('/api/start', methods=['POST'])
@app.route('/api/start-quiz', methods=['POST'])  # Alias para compatibilidad con frontend
def start_quiz():
    """
    Initialize a new quiz session and generate the first question.
    
    Expected JSON body:
    {
        "user_name": "Karem",
        "total_questions": 7
    }
    
    Returns:
    {
        "success": true,
        "session_id": "uuid",
        "message": "Greeting + first question",
        "options": ["opt1", "opt2", ...],
        "current_question": 1,
        "total_questions": 7
    }
    """
    global rag_service
    
    data = request.get_json()
    user_name = data.get('user_name', 'Mi Amor')
    total_questions = data.get('total_questions', 7)
    
    # Create new session
    session_id = str(uuid.uuid4())
    
    print(f"\n{'='*60}")
    print(f"üéØ Nueva sesi√≥n iniciada: {session_id}")
    print(f"{'='*60}")
    
    # Asegurar que RAG est√© inicializado
    current_rag = ensure_rag_initialized()
    if not current_rag:
        return jsonify({
            "success": False,
            "error": "No se pudo inicializar el sistema RAG. Verifique la configuraci√≥n."
        }), 500
    
    # ü§ñ Generar primera pregunta con OpenAI + RAG
    print(f"ü§ñ Generando pregunta #1 para {user_name}...")
    
    # Obtener mensajes de muestra para contexto inicial (ya no necesario con RAG, pero lo dejamos por compatibilidad)
    messages_sample = []
    
    first_question = generate_single_question_with_openai(
        messages_sample,
        question_number=1,
        previous_questions=None
    )
    
    if not first_question or not first_question.get('question'):
        return jsonify({
            "success": False,
            "error": "No se pudo generar la primera pregunta. Sistema RAG requerido para preguntas personalizadas."
        }), 500
    
    # Inicializar sesi√≥n
    quiz_sessions[session_id] = {
        "user_name": user_name,
        "total_questions": total_questions,
        "questions_asked": [first_question],
        "current_question_index": 0,
        "correct_answers": 0,
        "questions_skipped": 0,
        "attempts_current_question": 0,
        "max_attempts_per_question": 3,
        "hints_used": 0,
        "answers_history": [],
        "messages": messages_sample,
        "completed": False,
        "started_at": datetime.now().isoformat()
    }
    
    greeting = (
        f"Hola {user_name}.\n\n"
        f"He preparado {total_questions} preguntas basadas en nuestras conversaciones. "
        f"Al completarlas, tengo algo que decirte.\n\n"
        f"Pregunta 1 de {total_questions}:\n\n"
        f"{first_question['question']}"
    )
    
    return jsonify({
        "success": True,
        "session_id": session_id,
        "message": greeting,
        "question": first_question['question'],  # Para frontend
        "options": first_question.get('options', []),
        "current_question": 1,
        "total_questions": total_questions,
        "attempts_left": 3
    })


@app.route('/api/answer', methods=['POST'])
@app.route('/api/chat', methods=['POST'])  # Alias para compatibilidad con frontend
def answer_question():
    """
    Process user's answer to the current question.
    
    Expected JSON body:
    {
        "session_id": "uuid",
        "message": "user's answer"
    }
    
    Returns:
    {
        "success": true,
        "message": "feedback + next question OR completion message",
        "options": [...] (if there's a next question),
        "current_question": number,
        "total_questions": number,
        "correct_answers": number,
        "is_correct": boolean,
        "completed": boolean
    }
    """
    data = request.get_json()
    session_id = data.get('session_id')
    user_message = data.get('message', '').strip().lower()
    
    if not session_id or session_id not in quiz_sessions:
        return jsonify({
            "success": False,
            "error": "Invalid session ID"
        }), 400
    
    session = quiz_sessions[session_id]
    
    if session['completed']:
        return jsonify({
            "success": False,
            "error": "Quiz already completed"
        }), 400
    
    current_index = session['current_question_index']
    questions_asked = session['questions_asked']
    total_questions = session['total_questions']
    
    # Get current question
    if current_index >= len(questions_asked):
        return jsonify({
            "success": False,
            "error": "No current question available"
        }), 400
    
    current_question = questions_asked[current_index]
    correct_answers = current_question.get('correct_answers', [])
    
    # Check if answer is correct (case-insensitive, flexible matching)
    is_correct = any(
        correct.lower() in user_message or user_message in correct.lower()
        for correct in correct_answers
    )
    
    if is_correct:
        # ‚úÖ RESPUESTA CORRECTA
        session['correct_answers'] += 1
        session['attempts_current_question'] = 0
        session['answers_history'].append({
            'question': current_question.get('question', ''),
            'answer': user_message,
            'correct': True,
            'attempts': session['attempts_current_question']
        })
        
        print(f"‚úÖ Respuesta correcta! Total: {session['correct_answers']}/{total_questions}")
        
        # üéâ CHECK IF QUIZ COMPLETED
        if session['correct_answers'] >= total_questions:
            session['completed'] = True
            
            # ü§ñ Generar mensaje de completaci√≥n conversacional con OpenAI
            try:
                from services.chatbot import generate_completion_message
                completion_message = generate_completion_message(
                    openai_client=openai_client,
                    session_info={
                        'correct_answers': session['correct_answers'],
                        'total_questions': total_questions,
                        'current_question': current_index + 1
                    },
                    rag_service=rag_service
                )
            except Exception as e:
                print(f"‚ùå Error generando mensaje de completaci√≥n: {e}")
                return jsonify({
                    "success": False,
                    "error": "No se pudo generar mensaje de completaci√≥n personalizado.",
                    "completed": True
                }), 500
            
            return jsonify({
                "success": True,
                "message": completion_message,
                "completed": True,
                "is_correct": True,
                "options": []
            })
        
        # ü§ñ GENERAR SIGUIENTE PREGUNTA
        next_question_number = current_index + 2
        print(f"ü§ñ Generando pregunta #{next_question_number}...")
        
        next_question = generate_single_question_with_openai(
            session['messages'],
            question_number=next_question_number,
            previous_questions=questions_asked
        )
        
        if not next_question or not next_question.get('question'):
            # Sin fallbacks - finalizar quiz si no se puede generar siguiente pregunta
            session['completed'] = True
            return jsonify({
                "success": False,
                "error": "No se pudo generar la siguiente pregunta. Quiz finalizado.",
                "completed": True,
                "message": "El quiz ha terminado debido a problemas t√©cnicos."
            }), 500
        
        session['questions_asked'].append(next_question)
        session['current_question_index'] += 1
        
        # ü§ñ Generar respuesta conversacional para respuesta correcta
        try:
            conversational_response = generate_conversational_response(
                openai_client=openai_client,
                context="",
                user_answer=user_message,
                is_correct=True,
                question_info=current_question,
                session_info={
                    'current_question': current_index + 1,
                    'total_questions': total_questions,
                    'correct_answers': session['correct_answers']
                },
                rag_service=rag_service
            )
            
            # Generar introducci√≥n para la siguiente pregunta
            from services.chatbot import generate_next_question_intro
            question_intro = generate_next_question_intro(
                openai_client=openai_client,
                next_question=next_question,
                session_info={
                    'current_question': current_index + 1,
                    'total_questions': total_questions,
                    'correct_answers': session['correct_answers']
                }
            )
            
            response_message = f"{conversational_response}\n\n{question_intro}\n\n{next_question['question']}"
            
        except Exception as e:
            print(f"‚ùå Error generando respuesta conversacional: {e}")
            return jsonify({
                "success": False,
                "error": "No se pudo generar respuesta conversacional personalizada.",
                "completed": True
            }), 500
        
        return jsonify({
            "success": True,
            "message": response_message,
            "options": next_question.get('options', []),
            "current_question": next_question_number,
            "total_questions": total_questions,
            "correct_answers": session['correct_answers'],
            "is_correct": True,
            "completed": False,
            "attempts_left": 3
        })
    
    else:
        # ‚ùå RESPUESTA INCORRECTA
        session['attempts_current_question'] += 1
        attempts = session['attempts_current_question']
        max_attempts = session.get('max_attempts_per_question', 3)
        attempts_left = max_attempts - attempts
        
        print(f"‚ùå Respuesta incorrecta. Intento {attempts}/{max_attempts}")
        
        # ü§ñ Generar respuesta conversacional para respuesta incorrecta
        try:
            conversational_response = generate_conversational_response(
                openai_client=openai_client,
                context="",
                user_answer=user_message,
                is_correct=False,
                question_info=current_question,
                session_info={
                    'current_question': current_index + 1,
                    'total_questions': total_questions,
                    'correct_answers': session['correct_answers']
                },
                rag_service=rag_service
            )
        except Exception as e:
            print(f"‚ùå Error generando respuesta conversacional: {e}")
            correct_answer = current_question.get('correct_answers', ['la respuesta correcta'])[0]
            conversational_response = f"No exactamente, mi amor. La respuesta correcta era: {correct_answer} üíï"
        
        # Obtener pista si hay disponible
        hints = current_question.get('hints', [])
        hint_text = ""
        if attempts > 0 and attempts <= len(hints):
            hint_text = f"\n\nüí° Pista: {hints[attempts - 1]}"
        
        # Verificar si agot√≥ los 3 intentos
        if attempts >= max_attempts:
            # ‚ö†Ô∏è AGOT√ì LOS INTENTOS - Cambiar de pregunta
            print(f"‚ö†Ô∏è Agot√≥ los {max_attempts} intentos. Cambiando de pregunta...")
            
            session['questions_skipped'] += 1
            session['attempts_current_question'] = 0
            session['answers_history'].append({
                'question': current_question.get('question', ''),
                'answer': user_message,
                'correct': False,
                'attempts': attempts,
                'skipped': True
            })
            
            # Verificar si a√∫n puede completar el quiz
            questions_remaining = total_questions - (session['correct_answers'] + session['questions_skipped'])
            
            if questions_remaining <= 0:
                # No puede completar el quiz
                session['completed'] = True
                return jsonify({
                    "success": True,
                    "message": "Has agotado los intentos disponibles.\n\nPuedes intentarlo de nuevo cuando gustes.",
                    "completed": True,
                    "is_correct": False,
                    "options": []
                })
            
            # ü§ñ GENERAR NUEVA PREGUNTA (reemplazo)
            next_question_number = len(questions_asked) + 1
            print(f"ü§ñ Generando pregunta de reemplazo #{next_question_number}...")
            
            new_question = generate_single_question_with_openai(
                session['messages'],
                question_number=next_question_number,
                previous_questions=questions_asked
            )
            
            if not new_question or not new_question.get('question'):
                # Sin fallbacks - finalizar quiz
                session['completed'] = True
                return jsonify({
                    "success": False,
                    "error": "No se pudo generar pregunta de reemplazo. Quiz finalizado.",
                    "completed": True
                }), 500
            
            session['questions_asked'].append(new_question)
            session['current_question_index'] += 1
            
            response_message = (
                f"Probemos con otra pregunta.\n\n"
                f"Pregunta {next_question_number} de {total_questions}:\n\n"
                f"{new_question['question']}"
            )
            
            return jsonify({
                "success": True,
                "message": response_message,
                "options": new_question.get('options', []),
                "current_question": next_question_number,
                "total_questions": total_questions,
                "correct_answers": session['correct_answers'],
                "is_correct": False,
                "completed": False,
                "attempts_left": 3,
                "question_skipped": True
            })
        
        # üí° DAR PISTA (a√∫n tiene intentos)
        hints = current_question.get('hints', [])
        
        # Obtener la pista correcta seg√∫n el intento
        if hints and len(hints) >= attempts:
            hint = hints[attempts - 1]  # Primera pista en intento 1, segunda en intento 2
        else:
            hint = "Piensa en nuestros momentos especiales... üí≠"
        
        session['hints_used'] += 1
        
        response_message = f"{conversational_response}{hint_text}\n\n¬°Te quedan {attempts_left} intentos!"
        
        # MANTENER LAS OPCIONES VISIBLES
        return jsonify({
            "success": True,
            "message": response_message,
            "options": current_question.get('options', []),  # ‚úÖ Opciones siguen visibles
            "current_question": current_index + 1,
            "total_questions": total_questions,
            "correct_answers": session['correct_answers'],
            "is_correct": False,
            "completed": False,
            "attempts_left": attempts_left,
            "hint_given": True
        })


@app.route('/api/get-location', methods=['POST'])
def get_location():
    """
    Reveal final location (only after all questions answered correctly).
    
    Expected JSON body:
    {
        "session_id": "uuid"
    }
    """
    data = request.get_json()
    session_id = data.get('session_id')
    
    if not session_id or session_id not in quiz_sessions:
        return jsonify({
            "success": False,
            "error": "Invalid session ID"
        }), 400
    
    session = quiz_sessions[session_id]
    
    if not session['completed']:
        return jsonify({
            "success": False,
            "error": "Complete all questions first"
        }), 403
    
    final_location = {
        "latitude": float(os.getenv('FINAL_LATITUDE', 19.4326)),
        "longitude": float(os.getenv('FINAL_LONGITUDE', -99.1332)),
        "address": os.getenv('FINAL_ADDRESS', 'Te espero en un lugar especial'),
        "message": (
            f"Karem Kiyomi Ramos,\n\n"
            "Has demostrado que conoces bien nuestra historia. "
            "Ahora, ¬øpodr√≠as venir a este lugar? "
            "Hay algo importante que quiero preguntarte."
        )
    }
    
    return jsonify({
        "success": True,
        **final_location
    })


if __name__ == '__main__':
    # üöÄ Inicializar RAG Service al inicio
    print("\n" + "="*60)
    print("üöÄ Inicializando Romantic AI Proposal System v3.0 - Dashboard Edition")
    print("="*60)
    print(f"üè∑Ô∏è  Build: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # Cargar todos los mensajes
        logger.info("üì• Cargando mensajes...")
        all_messages = load_all_messages()
        logger.info(f"‚úÖ {len(all_messages)} mensajes cargados")
        
        # Inicializar RAG
        logger.info("üì° Inicializando RAG Service...")
        print("\nüì° Inicializando RAG Service...")
        rag_service = get_rag_service(os.getenv('OPENAI_API_KEY'))
        logger.info("‚úÖ RAG Service creado")
        
        # Construir √≠ndice (o cargar desde cache)
        logger.info("üî® Construyendo √≠ndice RAG...")
        rag_service.build_index(all_messages, force_rebuild=False)
        logger.info("‚úÖ √çndice RAG construido")
        
        # Mostrar estad√≠sticas
        stats = rag_service.get_statistics()
        logger.info(f"üìä RAG Stats - Chunks: {stats.get('total_chunks', 0):,}")
        print("\nüìä Estad√≠sticas del RAG:")
        print(f"  - Total chunks: {stats.get('total_chunks', 0):,}")
        print(f"  - Total mensajes: {stats.get('total_messages', 0):,}")
        print(f"  - Total vectores: {stats.get('total_vectors', 0):,}")
        print(f"  - Modelo embeddings: {stats.get('embedding_model', 'N/A')}")
        print(f"  - Dimensi√≥n: {stats.get('embedding_dimension', 0)}")
        print(f"  - Tama√±o √≠ndice: {stats.get('index_size_mb', 0):.2f} MB")
        print(f"  - Cache existe: {'‚úÖ' if stats.get('cache_exists', False) else '‚ùå'}")
        
        print("\n" + "="*60)
        print("‚úÖ Sistema inicializado correctamente")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\n‚ùå Error inicializando sistema: {e}")
        import traceback
        traceback.print_exc()
        print("\n‚ö†Ô∏è  El sistema continuar√° sin RAG, usando m√©todo b√°sico.")
    
    # Iniciar servidor
    # DigitalOcean usa puerto 8080 por defecto para health checks
    port = int(os.getenv('PORT', os.getenv('BACKEND_PORT', 8080)))
    host = os.getenv('HOST', '0.0.0.0')
    
    logger.info(f"üåê Servidor iniciando en http://{host}:{port}")
    logger.info(f"üîß Modo debug: {app.config['DEBUG']}")
    logger.info(f"üîë OpenAI API configurado: {'‚úÖ' if os.getenv('OPENAI_API_KEY') else '‚ùå'}")
    
    print(f"\nüåê Servidor iniciando en http://{host}:{port}")
    print(f"üîß Modo debug: {app.config['DEBUG']}")
    print(f"üîë OpenAI API configurado: {'‚úÖ' if os.getenv('OPENAI_API_KEY') else '‚ùå'}")
    
    if not os.getenv('OPENAI_API_KEY') or os.getenv('OPENAI_API_KEY') == 'your_openai_api_key_here':
        logger.warning("‚ö†Ô∏è OpenAI API Key no configurado correctamente")
        print("‚ö†Ô∏è  ADVERTENCIA: OpenAI API Key no configurado correctamente")
        print("   Configura OPENAI_API_KEY en el archivo .env")
    
    # Usar servidor de producci√≥n si no estamos en debug
    if app.config['DEBUG']:
        logger.info("üîß Iniciando servidor de desarrollo...")
        app.run(host=host, port=port, debug=True)
    else:
        # En producci√≥n, usar waitress (servidor WSGI m√°s robusto)
        logger.info("üöÄ Iniciando servidor de producci√≥n con Waitress...")
        print("üöÄ Iniciando servidor de producci√≥n con Waitress...")
        try:
            from waitress import serve
            serve(app, host=host, port=port, threads=4)
        except ImportError:
            logger.warning("‚ö†Ô∏è Waitress no disponible, usando servidor Flask")
            print("‚ö†Ô∏è Waitress no disponible, usando servidor Flask")
            app.run(host=host, port=port, debug=False, threaded=True)