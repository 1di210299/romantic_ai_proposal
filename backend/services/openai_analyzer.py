"""
Backend service para analizar mensajes usando OpenAI API.
Procesa chunks de mensajes y genera preguntas personalizadas automÃ¡ticamente.
"""

import os
import json
from typing import Dict, List, Optional
from datetime import datetime
from openai import OpenAI
from pathlib import Path
import time


class OpenAIMessageAnalyzer:
    """Analizador que usa OpenAI para procesar mensajes y generar preguntas."""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize analyzer with OpenAI.
        
        Args:
            api_key: OpenAI API key (si no se provee, usa variable de entorno)
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API key no encontrada. Configura OPENAI_API_KEY")
        
        self.client = OpenAI(api_key=self.api_key)
        self.analysis_results = {
            'insights': [],
            'questions': [],
            'timeline': {},
            'relationship_context': {}
        }
    
    def load_messages(self, conversation_path: str) -> List[Dict]:
        """Carga todos los mensajes de la conversaciÃ³n."""
        print("\nğŸ“‚ Cargando mensajes...")
        
        path = Path(conversation_path)
        all_messages = []
        
        for msg_file in sorted(path.glob("message_*.json")):
            print(f"   Leyendo {msg_file.name}...")
            with open(msg_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                messages = data.get('messages', [])
                all_messages.extend(messages)
        
        # Ordenar por timestamp
        all_messages.sort(key=lambda x: x.get('timestamp_ms', 0))
        
        print(f"âœ… Total: {len(all_messages):,} mensajes cargados\n")
        return all_messages
    
    def create_message_chunks(self, messages: List[Dict], chunk_size: int = 100) -> List[str]:
        """
        Crea chunks de mensajes en formato texto para OpenAI.
        
        Args:
            messages: Lista de mensajes
            chunk_size: NÃºmero de mensajes por chunk
        
        Returns:
            Lista de chunks en formato texto
        """
        print(f"ğŸ“¦ Creando chunks de {chunk_size} mensajes...")
        
        chunks = []
        
        for i in range(0, len(messages), chunk_size):
            chunk_messages = messages[i:i + chunk_size]
            
            # Convertir a formato legible para OpenAI
            chunk_text = self._format_messages_for_ai(chunk_messages)
            chunks.append(chunk_text)
        
        print(f"âœ… {len(chunks)} chunks creados\n")
        return chunks
    
    def _format_messages_for_ai(self, messages: List[Dict]) -> str:
        """Formatea mensajes para que sean legibles por OpenAI."""
        formatted = []
        
        for msg in messages:
            sender = msg.get('sender_name', 'Unknown')
            content = msg.get('content', '')
            timestamp = msg.get('timestamp_ms', 0)
            
            if content:
                date = datetime.fromtimestamp(timestamp / 1000).strftime('%Y-%m-%d')
                formatted.append(f"[{date}] {sender}: {content}")
        
        return "\n".join(formatted)
    
    def analyze_chunk_with_openai(self, chunk_text: str, chunk_id: int) -> Dict:
        """
        Analiza un chunk de mensajes usando OpenAI.
        
        Args:
            chunk_text: Texto del chunk
            chunk_id: ID del chunk
        
        Returns:
            Dict con insights del chunk
        """
        print(f"ğŸ¤– Analizando chunk {chunk_id + 1} con OpenAI...")
        
        prompt = f"""Analiza esta conversaciÃ³n de una pareja y extrae informaciÃ³n relevante para crear un quiz romÃ¡ntico.

CONVERSACIÃ“N:
{chunk_text}

EXTRAE:
1. Lugares mencionados (cafÃ©s, restaurantes, parques, etc.)
2. Fechas o eventos importantes mencionados
3. Apodos cariÃ±osos usados
4. Actividades que hacen juntos
5. Momentos especiales o significativos
6. Cualquier cosa Ãºnica de su relaciÃ³n

Responde en formato JSON con esta estructura:
{{
    "lugares": ["lista de lugares mencionados"],
    "fechas_eventos": ["eventos importantes mencionados"],
    "apodos": ["apodos cariÃ±osos"],
    "actividades": ["actividades compartidas"],
    "momentos_especiales": ["momentos significativos"],
    "frases_unicas": ["frases o expresiones Ãºnicas de ellos"]
}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # MÃ¡s barato y rÃ¡pido
                messages=[
                    {
                        "role": "system",
                        "content": "Eres un experto en analizar conversaciones de parejas para identificar momentos significativos y crear experiencias personalizadas."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            result['chunk_id'] = chunk_id
            result['tokens_used'] = response.usage.total_tokens
            
            print(f"   âœ“ Chunk {chunk_id + 1} analizado ({result['tokens_used']} tokens)")
            
            return result
            
        except Exception as e:
            print(f"   âœ— Error en chunk {chunk_id + 1}: {e}")
            return {
                'chunk_id': chunk_id,
                'error': str(e),
                'lugares': [],
                'fechas_eventos': [],
                'apodos': [],
                'actividades': [],
                'momentos_especiales': [],
                'frases_unicas': []
            }
    
    def merge_chunk_insights(self, chunk_results: List[Dict]) -> Dict:
        """Combina insights de todos los chunks."""
        print("\nğŸ”„ Combinando insights de todos los chunks...")
        
        merged = {
            'lugares': [],
            'fechas_eventos': [],
            'apodos': [],
            'actividades': [],
            'momentos_especiales': [],
            'frases_unicas': [],
            'total_tokens': 0
        }
        
        for result in chunk_results:
            if 'error' not in result:
                merged['lugares'].extend(result.get('lugares', []))
                merged['fechas_eventos'].extend(result.get('fechas_eventos', []))
                merged['apodos'].extend(result.get('apodos', []))
                merged['actividades'].extend(result.get('actividades', []))
                merged['momentos_especiales'].extend(result.get('momentos_especiales', []))
                merged['frases_unicas'].extend(result.get('frases_unicas', []))
                merged['total_tokens'] += result.get('tokens_used', 0)
        
        # Eliminar duplicados y ordenar por frecuencia
        for key in ['lugares', 'apodos', 'actividades', 'frases_unicas']:
            if merged[key]:
                # Contar frecuencia
                from collections import Counter
                counter = Counter(merged[key])
                # Ordenar por frecuencia
                merged[key] = [item for item, count in counter.most_common()]
        
        print(f"âœ… Insights combinados ({merged['total_tokens']} tokens totales)")
        
        return merged
    
    def generate_questions_with_openai(self, insights: Dict, messages_sample: List[Dict]) -> List[Dict]:
        """
        Genera preguntas personalizadas usando OpenAI basadas en los insights.
        
        Args:
            insights: Insights extraÃ­dos de los mensajes
            messages_sample: Muestra de mensajes para contexto adicional
        
        Returns:
            Lista de preguntas generadas
        """
        print("\nğŸ’¡ Generando preguntas personalizadas con OpenAI...")
        
        # Obtener contexto temporal
        first_msg = messages_sample[0] if messages_sample else None
        last_msg = messages_sample[-1] if messages_sample else None
        
        first_date = datetime.fromtimestamp(first_msg['timestamp_ms'] / 1000).strftime('%Y-%m-%d') if first_msg else "desconocida"
        last_date = datetime.fromtimestamp(last_msg['timestamp_ms'] / 1000).strftime('%Y-%m-%d') if last_msg else "desconocida"
        
        prompt = f"""Eres un experto en crear experiencias romÃ¡nticas personalizadas. 

INFORMACIÃ“N DE LA RELACIÃ“N:
- Primera conversaciÃ³n: {first_date}
- Ãšltima conversaciÃ³n: {last_date}
- Total de mensajes: {len(messages_sample):,}

INSIGHTS EXTRAÃDOS:
{json.dumps(insights, indent=2, ensure_ascii=False)}

MUESTRA DE CONVERSACIONES RECIENTES:
{self._format_messages_for_ai(messages_sample[-50:])}

TAREA:
Genera 7 preguntas personalizadas para un quiz romÃ¡ntico que llevarÃ¡ a una propuesta especial.
Las preguntas deben:
1. Ser especÃ­ficas de esta relaciÃ³n
2. Variar en dificultad (2 fÃ¡ciles, 3 medias, 2 difÃ­ciles)
3. Tener respuestas basadas en datos reales
4. Incluir 3 pistas progresivas si se equivoca
5. Tener un mensaje de Ã©xito romÃ¡ntico

Responde en formato JSON:
{{
  "questions": [
    {{
      "id": 1,
      "question": "pregunta especÃ­fica",
      "category": "categorÃ­a (lugares/fechas/apodos/momentos/actividades)",
      "difficulty": "easy/medium/hard",
      "correct_answers": ["respuesta correcta", "variaciÃ³n 1", "variaciÃ³n 2"],
      "hints": ["pista 1 sutil", "pista 2 mÃ¡s clara", "pista 3 casi la respuesta"],
      "success_message": "mensaje romÃ¡ntico al acertar",
      "context": "contexto para el chatbot (no se muestra al usuario)",
      "why_important": "por quÃ© esta pregunta es significativa para ellos"
    }}
  ]
}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",  # Modelo mÃ¡s inteligente para preguntas de calidad
                messages=[
                    {
                        "role": "system",
                        "content": "Eres un experto en crear experiencias romÃ¡nticas memorables y personalizadas. Conoces cÃ³mo hacer preguntas significativas que conecten emocionalmente."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,  # Un poco mÃ¡s creativo
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            questions = result.get('questions', [])
            tokens_used = response.usage.total_tokens
            
            print(f"âœ… {len(questions)} preguntas generadas ({tokens_used} tokens)")
            print(f"ğŸ’° Costo aproximado: ${tokens_used * 0.000005:.4f}")
            
            return questions
            
        except Exception as e:
            print(f"âœ— Error generando preguntas: {e}")
            return []
    
    def create_chatbot_context(self, insights: Dict, messages_sample: List[Dict]) -> Dict:
        """
        Crea contexto enriquecido para el chatbot usando OpenAI.
        
        Args:
            insights: Insights de la relaciÃ³n
            messages_sample: Muestra de mensajes
        
        Returns:
            Contexto para el chatbot
        """
        print("\nğŸ¤– Generando contexto para el chatbot...")
        
        prompt = f"""Analiza esta relaciÃ³n y crea un contexto personalizado para un chatbot romÃ¡ntico.

INSIGHTS:
{json.dumps(insights, indent=2, ensure_ascii=False)}

MUESTRA DE CONVERSACIONES:
{self._format_messages_for_ai(messages_sample[-100:])}

Crea un perfil de la relaciÃ³n que incluya:
1. Estilo de comunicaciÃ³n de Ã©l
2. Temas recurrentes en sus conversaciones
3. Nivel de formalidad/informalidad
4. Emojis o expresiones favoritas
5. Tono general (divertido, serio, cariÃ±oso, etc.)

Responde en JSON:
{{
    "communication_style": "descripciÃ³n del estilo",
    "recurring_themes": ["tema 1", "tema 2", ...],
    "formality_level": "formal/casual/muy_casual",
    "favorite_expressions": ["expresiÃ³n 1", "expresiÃ³n 2", ...],
    "overall_tone": "descripciÃ³n del tono",
    "relationship_personality": "descripciÃ³n general de la relaciÃ³n"
}}"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "Eres un experto en anÃ¡lisis de comunicaciÃ³n interpersonal."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            context = json.loads(response.choices[0].message.content)
            tokens_used = response.usage.total_tokens
            
            print(f"âœ… Contexto generado ({tokens_used} tokens)")
            
            return context
            
        except Exception as e:
            print(f"âœ— Error generando contexto: {e}")
            return {}
    
    def analyze_complete(
        self,
        conversation_path: str,
        max_messages: Optional[int] = None,
        chunk_size: int = 100
    ) -> Dict:
        """
        Ejecuta anÃ¡lisis completo de mensajes con OpenAI.
        
        Args:
            conversation_path: Ruta a la carpeta de mensajes
            max_messages: LÃ­mite de mensajes (None = todos)
            chunk_size: TamaÃ±o de cada chunk
        
        Returns:
            Dict con anÃ¡lisis completo
        """
        print("\n" + "="*70)
        print("ğŸš€ ANÃLISIS COMPLETO CON OPENAI")
        print("="*70)
        
        start_time = time.time()
        total_cost = 0
        
        # 1. Cargar mensajes
        all_messages = self.load_messages(conversation_path)
        
        if max_messages:
            print(f"âš ï¸  Limitando anÃ¡lisis a {max_messages} mensajes")
            all_messages = all_messages[:max_messages]
        
        # 2. Crear chunks
        chunks = self.create_message_chunks(all_messages, chunk_size)
        
        print(f"ğŸ“Š Se analizarÃ¡n {len(chunks)} chunks")
        print(f"ğŸ’° Costo estimado: ${len(chunks) * 500 * 0.000005:.2f} - ${len(chunks) * 1000 * 0.000005:.2f}\n")
        
        confirm = input("Â¿Continuar con el anÃ¡lisis? (s/n): ").strip().lower()
        if confirm != 's':
            print("âŒ AnÃ¡lisis cancelado")
            return {}
        
        # 3. Analizar chunks con OpenAI
        chunk_results = []
        for i, chunk in enumerate(chunks):
            result = self.analyze_chunk_with_openai(chunk, i)
            chunk_results.append(result)
            
            # PequeÃ±a pausa para no saturar la API
            if i < len(chunks) - 1:
                time.sleep(0.5)
        
        # 4. Combinar insights
        merged_insights = self.merge_chunk_insights(chunk_results)
        
        # 5. Generar preguntas
        questions = self.generate_questions_with_openai(merged_insights, all_messages)
        
        # 6. Crear contexto para chatbot
        chatbot_context = self.create_chatbot_context(merged_insights, all_messages)
        
        # 7. Compilar resultados
        elapsed_time = time.time() - start_time
        
        result = {
            'metadata': {
                'analyzed_at': datetime.now().isoformat(),
                'total_messages_analyzed': len(all_messages),
                'chunks_processed': len(chunks),
                'chunk_size': chunk_size,
                'total_tokens_used': merged_insights['total_tokens'],
                'processing_time_seconds': round(elapsed_time, 2),
                'estimated_cost_usd': round(merged_insights['total_tokens'] * 0.000005, 4)
            },
            'timeline': {
                'first_message': datetime.fromtimestamp(all_messages[0]['timestamp_ms'] / 1000).isoformat(),
                'last_message': datetime.fromtimestamp(all_messages[-1]['timestamp_ms'] / 1000).isoformat(),
                'total_messages': len(all_messages)
            },
            'insights': merged_insights,
            'questions': questions,
            'chatbot_context': chatbot_context
        }
        
        return result
    
    def save_results(self, result: Dict, output_path: str = "data/openai_analysis.json"):
        """Guarda resultados del anÃ¡lisis."""
        print(f"\nğŸ’¾ Guardando resultados en {output_path}...")
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Resultados guardados\n")
    
    def export_questions_to_template(self, questions: List[Dict], output_path: str = "data/questions_generated.json"):
        """Exporta preguntas generadas al formato del template."""
        print(f"\nğŸ“ Exportando preguntas a {output_path}...")
        
        template = {
            "quiz_metadata": {
                "title": "Nuestro Rally del Amor ğŸ’•",
                "description": "Un viaje por nuestros momentos especiales",
                "created_by": "Juan Diego Gutierrez",
                "created_for": "Karem Ramos",
                "total_questions": len(questions),
                "generated_with": "OpenAI GPT-4",
                "generated_at": datetime.now().isoformat()
            },
            "questions": questions
        }
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(template, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Preguntas exportadas\n")


def print_summary(result: Dict):
    """Imprime resumen del anÃ¡lisis."""
    print("\n" + "="*70)
    print("ğŸ“Š RESUMEN DEL ANÃLISIS CON OPENAI")
    print("="*70)
    
    metadata = result['metadata']
    insights = result['insights']
    questions = result['questions']
    
    print(f"\nâ±ï¸  PROCESAMIENTO:")
    print(f"   Tiempo: {metadata['processing_time_seconds']} segundos")
    print(f"   Mensajes analizados: {metadata['total_messages_analyzed']:,}")
    print(f"   Chunks procesados: {metadata['chunks_processed']}")
    print(f"   Tokens usados: {metadata['total_tokens_used']:,}")
    print(f"   ğŸ’° Costo: ${metadata['estimated_cost_usd']}")
    
    print(f"\nğŸ” INSIGHTS DESCUBIERTOS:")
    print(f"   Lugares: {len(insights.get('lugares', []))}")
    print(f"   Eventos/Fechas: {len(insights.get('fechas_eventos', []))}")
    print(f"   Apodos: {len(insights.get('apodos', []))}")
    print(f"   Actividades: {len(insights.get('actividades', []))}")
    print(f"   Momentos especiales: {len(insights.get('momentos_especiales', []))}")
    
    if insights.get('lugares'):
        print(f"\nğŸ“ TOP LUGARES:")
        for i, lugar in enumerate(insights['lugares'][:5], 1):
            print(f"   {i}. {lugar}")
    
    if insights.get('apodos'):
        print(f"\nğŸ’• APODOS:")
        for i, apodo in enumerate(insights['apodos'][:5], 1):
            print(f"   {i}. {apodo}")
    
    print(f"\nğŸ’¡ PREGUNTAS GENERADAS: {len(questions)}")
    for i, q in enumerate(questions, 1):
        print(f"\n   {i}. [{q.get('difficulty', 'medium').upper()}] {q.get('question', '')}")
        print(f"      CategorÃ­a: {q.get('category', '')}")
        print(f"      Respuestas: {', '.join(q.get('correct_answers', [])[:2])}")


def main():
    """FunciÃ³n principal."""
    print("\n" + "="*70)
    print("ğŸ’• ROMANTIC AI - ANÃLISIS CON OPENAI")
    print("   Genera preguntas personalizadas automÃ¡ticamente")
    print("="*70 + "\n")
    
    # Verificar API key
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ Error: OPENAI_API_KEY no encontrada")
        print("\nğŸ“ Configura tu API key:")
        print("   export OPENAI_API_KEY='tu-api-key'")
        print("   O crÃ©ala en el archivo .env\n")
        return
    
    print("âœ… API key encontrada\n")
    
    # ConfiguraciÃ³n
    conversation_path = "karemramos_1184297046409691"
    
    if not os.path.exists(conversation_path):
        print(f"âŒ Error: Carpeta {conversation_path} no encontrada")
        return
    
    # Preguntar lÃ­mite de mensajes
    print("âš™ï¸  CONFIGURACIÃ“N:")
    print("   Para pruebas rÃ¡pidas, limita los mensajes a analizar")
    print("   Para anÃ¡lisis completo, deja en blanco\n")
    
    max_msg_input = input("   MÃ¡ximo de mensajes (ej: 1000, o ENTER para todos): ").strip()
    max_messages = int(max_msg_input) if max_msg_input else None
    
    chunk_input = input("   TamaÃ±o de chunk (default: 100, ENTER): ").strip()
    chunk_size = int(chunk_input) if chunk_input else 100
    
    print()
    
    # Inicializar analizador
    try:
        analyzer = OpenAIMessageAnalyzer(api_key)
    except ValueError as e:
        print(f"âŒ Error: {e}")
        return
    
    # Ejecutar anÃ¡lisis
    result = analyzer.analyze_complete(
        conversation_path=conversation_path,
        max_messages=max_messages,
        chunk_size=chunk_size
    )
    
    if not result:
        return
    
    # Guardar resultados
    analyzer.save_results(result)
    
    # Exportar preguntas
    if result.get('questions'):
        analyzer.export_questions_to_template(result['questions'])
    
    # Mostrar resumen
    print_summary(result)
    
    print("\n" + "="*70)
    print("âœ… ANÃLISIS COMPLETADO")
    print("="*70)
    
    print(f"\nğŸ“„ ARCHIVOS GENERADOS:")
    print(f"   1. data/openai_analysis.json - AnÃ¡lisis completo")
    print(f"   2. data/questions_generated.json - Preguntas listas para usar")
    
    print(f"\nğŸ¯ PRÃ“XIMOS PASOS:")
    print(f"   1. Revisa las preguntas generadas")
    print(f"   2. Personaliza si es necesario")
    print(f"   3. Copia a data/questions.json")
    print(f"   4. Configura el backend")
    print(f"   5. Â¡Prueba el chatbot!\n")


if __name__ == "__main__":
    main()
